---
title: Ollama
description: 使用 Ollama 在本地运行开源模型，无需 API Key。
---

# Ollama

Ollama 让你在本地运行开源大语言模型，完全离线，无需 API Key，数据不离开本机。

## 安装 Ollama / Install Ollama

从 [ollama.com](https://ollama.com/) 下载并安装 Ollama。

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: 下载安装程序
```

## 下载模型 / Pull Models

```bash
# 下载 Llama 3.2
ollama pull llama3.2

# 下载 Qwen2.5
ollama pull qwen2.5

# 下载 DeepSeek-R1（7B）
ollama pull deepseek-r1:7b

# 查看已下载的模型
ollama list
```

## 在 OpenCowork 中配置 / Configure

进入 **设置 → AI 提供商**，添加新提供商：

| 配置项 | 值 |
|--------|-----|
| 协议 | `openai-chat` |
| Base URL | `http://localhost:11434/v1` |
| API Key | `ollama`（任意字符串） |

## 推荐模型 / Recommended Models

| 模型 | 大小 | 适用场景 |
|------|------|---------|
| `llama3.2:3b` | 2GB | 快速响应，低配机器 |
| `llama3.2:8b` | 5GB | 通用对话 |
| `qwen2.5:7b` | 4.7GB | 中文优化 |
| `deepseek-r1:7b` | 4.7GB | 推理任务 |
| `codellama:13b` | 7.4GB | 代码生成 |

<Callout type="tip">
  Ollama 默认监听 `http://localhost:11434`。如果修改了端口，记得同步更新 OpenCowork 中的 Base URL。
</Callout>
