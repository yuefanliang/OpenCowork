"""
å°çº¢ä¹¦æœç´¢çˆ¬å–è„šæœ¬
è‡ªåŠ¨æœç´¢å…³é”®è¯å¹¶æå–ç¬”è®°å›¾ç‰‡
"""

import asyncio
import argparse
import os
import sys
from pathlib import Path
from playwright.async_api import async_playwright, Page


def get_default_chrome_user_data_dir():
    """è·å– Windows ç³»ç»Ÿé»˜è®¤æµè§ˆå™¨ç”¨æˆ·æ•°æ®ç›®å½•"""
    local_app_data = os.environ.get('LOCALAPPDATA')
    if not local_app_data:
        return None
    
    chrome_path = Path(local_app_data) / "Google" / "Chrome" / "User Data"
    if chrome_path.exists():
        return str(chrome_path)
    
    edge_path = Path(local_app_data) / "Microsoft" / "Edge" / "User Data"
    if edge_path.exists():
        return str(edge_path)
    
    return None


def show_login_notification():
    """æ˜¾ç¤ºç™»å½•æé†’"""
    try:
        import ctypes
        ctypes.windll.user32.MessageBoxW(
            0,
            "æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€\n\nè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆå°çº¢ä¹¦ç™»å½•ï¼Œç„¶åå…³é—­æµè§ˆå™¨çª—å£ç»§ç»­",
            "éœ€è¦ç™»å½•å°çº¢ä¹¦",
            0x40 | 0x0
        )
    except Exception:
        print("\n" + "="*50)
        print("âš ï¸  è¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦...")
        print("="*50 + "\n")


async def check_login(page: Page, timeout=5000):
    """æ£€æŸ¥å°çº¢ä¹¦ç™»å½•çŠ¶æ€ - æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ç™»å½•æŒ‡ç¤ºå™¨"""
    try:
        # æ£€æŸ¥å¤šç§ç™»å½•æŒ‡ç¤ºå™¨ï¼ˆæ ¹æ®ç”¨æˆ·æä¾›çš„HTMLæ›´æ–°ï¼‰
        selectors = [
            ".user.side-bar-component",                  # ç”¨æˆ·å¤´åƒå¯¼èˆªé¡¹
            "a[href*='/user/profile']",                 # ä¸ªäººä¸»é¡µé“¾æ¥
            "[src*='sns-avatar']",                      # å°çº¢ä¹¦å¤´åƒå›¾ç‰‡
            "[class*='user']",                          # åŒ…å« user çš„ç±»
            ".avatar",                                  # å¤´åƒ
            "[class*='avatar']",                        # åŒ…å« avatar çš„ç±»
            "img[src*='avatar']",                       # å¤´åƒå›¾ç‰‡
            ".user-nickname",                           # ç”¨æˆ·å
            "text=åˆ›ä½œä¸­å¿ƒ",                            # åˆ›ä½œä¸­å¿ƒæŒ‰é’®
            "text=ä¸šåŠ¡åˆä½œ",                            # ä¸šåŠ¡åˆä½œæŒ‰é’®
            ".publish-btn",                             # å‘å¸ƒæŒ‰é’®
            ".global-nav"                               # å…¨å±€å¯¼èˆª
        ]
        
        for selector in selectors:
            try:
                await page.wait_for_selector(selector, timeout=2000)
                print(f"âœ… æ£€æµ‹åˆ°ç™»å½•å…ƒç´ : {selector}")
                return True
            except:
                continue
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡é¡µé¢å†…å®¹åˆ¤æ–­
        html = await page.content()
        login_indicators = ['é€€å‡ºç™»å½•', 'ä¸ªäººä¸»é¡µ', 'åˆ›ä½œä¸­å¿ƒ', 'ä¸šåŠ¡åˆä½œ', 'sns-avatar', '/user/profile/']
        for indicator in login_indicators:
            if indicator in html:
                print(f"âœ… é€šè¿‡é¡µé¢å†…å®¹æ£€æµ‹åˆ°å·²ç™»å½•: {indicator}")
                return True
            
        return False
    except:
        return False


async def wait_for_login(page: Page):
    """ç­‰å¾…ç”¨æˆ·ç™»å½•"""
    show_login_notification()
    print("â³ ç­‰å¾…ç™»å½•ä¸­...ï¼ˆæœ€å¤š 5 åˆ†é’Ÿï¼‰")
    print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åç‚¹å‡»ç¡®å®šæŒ‰é’®ç»§ç»­")
    
    # ç­‰å¾…å¼¹çª—å…³é—­åç»§ç»­æ£€æµ‹
    import time
    start_time = time.time()
    while time.time() - start_time < 300:
        if await check_login(page, timeout=3000):
            print("âœ… ç™»å½•æˆåŠŸï¼")
            return True
        await asyncio.sleep(2)
    
    print("âŒ ç™»å½•è¶…æ—¶")
    return False


async def crawl_xiaohongshu(keyword: str, count: int = 20, save_dir: str = None):
    """çˆ¬å–å°çº¢ä¹¦æœç´¢ç»“æœ"""
    
    user_data_dir = get_default_chrome_user_data_dir()
    
    # æ„å»ºæœç´¢ URL
    search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}&type=51"
    
    async with async_playwright() as p:
        if user_data_dir:
            print(f"ğŸ“ ä½¿ç”¨ç³»ç»Ÿæµè§ˆå™¨")
            browser = await p.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=False,
                args=["--disable-blink-features=AutomationControlled"]
            )
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç³»ç»Ÿæµè§ˆå™¨ï¼Œä½¿ç”¨ä¸´æ—¶é…ç½®")
            browser = await p.chromium.launch(headless=False)
        
        try:
            page = await browser.new_page()
            
            print(f"ğŸ” æœç´¢: {keyword}")
            await page.goto(search_url, wait_until="networkidle", timeout=60000)
            
            # ç­‰å¾…åŠ è½½
            await asyncio.sleep(3)
            
            # æ£€æŸ¥ç™»å½•
            is_logged_in = await check_login(page)
            if not is_logged_in:
                print("ğŸ” éœ€è¦ç™»å½•å°çº¢ä¹¦")
                success = await wait_for_login(page)
                if not success:
                    raise Exception("æœªå®Œæˆç™»å½•")
                await page.goto(search_url, wait_until="networkidle", timeout=60000)
                await asyncio.sleep(3)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤š
            print("ğŸ“œ åŠ è½½æ›´å¤šå†…å®¹...")
            for _ in range(5):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
            
            # æå–å›¾ç‰‡
            print("ğŸ–¼ï¸ æå–å›¾ç‰‡ä¸­...")
            
            # å°çº¢ä¹¦å›¾ç‰‡é€‰æ‹©å™¨ - å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            img_elements = await page.query_selector_all(
                '.note-item img, .cover-img img, .img img, [class*="cover"] img, .item img'
            )
            
            results = []
            for i, img in enumerate(img_elements[:count]):
                try:
                    src = await img.get_attribute('src')
                    if src and ('http' in src or '//' in src):
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if src.startswith('//'):
                            src = 'https:' + src
                        results.append(src)
                        print(f"  [{i+1}] {src[:80]}...")
                except Exception as e:
                    continue
            
            # æå–æ ‡é¢˜
            title_elements = await page.query_selector_all(
                '.note-item .title, .title-content, [class*="title"]'
            )
            titles = []
            for title in title_elements[:count]:
                try:
                    text = await title.inner_text()
                    if text:
                        titles.append(text.strip())
                except:
                    pass
            
            # è¾“å‡ºç»“æœ
            print(f"\n{'='*50}")
            print(f"ğŸ“Š çˆ¬å–ç»“æœ: å…± {len(results)} å¼ å›¾ç‰‡")
            print(f"{'='*50}")
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            if save_dir:
                save_path = Path(save_dir)
                save_path.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜å›¾ç‰‡é“¾æ¥
                links_file = save_path / f"{keyword}_links.txt"
                with open(links_file, "w", encoding="utf-8") as f:
                    f.write(f"# å°çº¢ä¹¦æœç´¢: {keyword}\n")
                    f.write(f"# å›¾ç‰‡æ•°é‡: {len(results)}\n\n")
                    for i, src in enumerate(results):
                        f.write(f"{i+1}. {src}\n")
                
                # ä¿å­˜æ ‡é¢˜
                if titles:
                    titles_file = save_path / f"{keyword}_titles.txt"
                    with open(titles_file, "w", encoding="utf-8") as f:
                        f.write(f"# å°çº¢ä¹¦æœç´¢: {keyword}\n\n")
                        for i, t in enumerate(titles):
                            f.write(f"{i+1}. {t}\n")
                
                print(f"ğŸ’¾ å·²ä¿å­˜é“¾æ¥åˆ°: {links_file}")
                print(f"ğŸ’¾ å·²ä¿å­˜æ ‡é¢˜åˆ°: {titles_file}")
            
            return results, titles
            
        finally:
            await browser.close()


def main():
    import json

    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦æœç´¢çˆ¬å–")
    parser.add_argument("keyword", help="æœç´¢å…³é”®è¯")
    parser.add_argument("--count", type=int, default=20, help="çˆ¬å–æ•°é‡")
    parser.add_argument("--save", help="ä¿å­˜ç›®å½•")

    args = parser.parse_args()

    results, titles = asyncio.run(crawl_xiaohongshu(
        keyword=args.keyword,
        count=args.count,
        save_dir=args.save
    ))

    # è¾“å‡ºç»“æ„åŒ– JSON ä¾›è°ƒç”¨æ–¹è§£æ
    output = {
        "keyword": args.keyword,
        "images": results,
        "titles": titles,
        "count": len(results)
    }
    print(json.dumps(output, ensure_ascii=False))


if __name__ == "__main__":
    main()
